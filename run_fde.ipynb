{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95c909e3-18eb-4dc7-82bd-5e7b111581a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting ir_datasets\n",
      "  Downloading ir_datasets-0.5.11-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting neural-cherche\n",
      "  Downloading neural_cherche-1.4.3.tar.gz (31 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.1.2)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2024.10.0)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.35.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (4.13.3)\n",
      "Collecting inscriptis>=2.2.0 (from ir_datasets)\n",
      "  Downloading inscriptis-2.6.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting lxml>=4.5.2 (from ir_datasets)\n",
      "  Downloading lxml-6.0.1-cp311-cp311-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting trec-car-tools>=2.5.4 (from ir_datasets)\n",
      "  Downloading trec_car_tools-2.6-py3-none-any.whl.metadata (640 bytes)\n",
      "Collecting lz4>=3.1.10 (from ir_datasets)\n",
      "  Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting warc3-wet>=0.2.3 (from ir_datasets)\n",
      "  Downloading warc3_wet-0.2.5-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting warc3-wet-clueweb09>=0.2.5 (from ir_datasets)\n",
      "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting zlib-state>=0.1.3 (from ir_datasets)\n",
      "  Downloading zlib_state-0.1.10-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting ijson>=3.1.3 (from ir_datasets)\n",
      "  Downloading ijson-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting unlzw3>=0.2.1 (from ir_datasets)\n",
      "  Downloading unlzw3-0.2.3-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.11/dist-packages (from neural-cherche) (2.8.0.dev20250319+cu128)\n",
      "Collecting transformers>=4.34.0 (from neural-cherche)\n",
      "  Downloading transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting lenlp>=1.1.1 (from neural-cherche)\n",
      "  Downloading lenlp-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.8 kB)\n",
      "Collecting scikit-learn>=1.5.0 (from neural-cherche)\n",
      "  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir_datasets) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir_datasets) (4.12.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.24.0->datasets)\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Collecting scipy>=1.13.1 (from lenlp>=1.1.1->neural-cherche)\n",
      "  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn>=1.5.0->neural-cherche)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.5.0->neural-cherche)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->neural-cherche) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->neural-cherche) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->neural-cherche) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->neural-cherche) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->neural-cherche) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->neural-cherche) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->neural-cherche) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->neural-cherche) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->neural-cherche) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->neural-cherche) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->neural-cherche) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->neural-cherche) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->neural-cherche) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->neural-cherche) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->neural-cherche) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->neural-cherche) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->neural-cherche) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->neural-cherche) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch>=1.13->neural-cherche) (77.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.34.0->neural-cherche)\n",
      "  Downloading regex-2025.9.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.34.0->neural-cherche)\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers>=4.34.0->neural-cherche)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting cbor>=1.0.0 (from trec-car-tools>=2.5.4->ir_datasets)\n",
      "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=1.13->neural-cherche) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13->neural-cherche) (2.1.5)\n",
      "Downloading datasets-4.1.1-py3-none-any.whl (503 kB)\n",
      "Downloading ir_datasets-0.5.11-py3-none-any.whl (866 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.1/866.1 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading huggingface_hub-0.35.0-py3-none-any.whl (563 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.4/563.4 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ijson-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (134 kB)\n",
      "Downloading inscriptis-2.6.0-py3-none-any.whl (45 kB)\n",
      "Downloading lenlp-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.2/773.2 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lxml-6.0.1-cp311-cp311-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
      "Downloading unlzw3-0.2.3-py3-none-any.whl (6.7 kB)\n",
      "Downloading warc3_wet-0.2.5-py3-none-any.whl (18 kB)\n",
      "Downloading zlib_state-0.1.10-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (21 kB)\n",
      "Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading regex-2025.9.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m799.0/799.0 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
      "Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n",
      "Building wheels for collected packages: neural-cherche, warc3-wet-clueweb09, cbor\n",
      "  Building wheel for neural-cherche (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for neural-cherche: filename=neural_cherche-1.4.3-py3-none-any.whl size=46836 sha256=117fe8d35cd85edd25c3ee8129808a08e9b7885c246936d43fd81cf55f591318\n",
      "  Stored in directory: /root/.cache/pip/wheels/05/52/03/08558ee86a492e067b55c092057516804e6224f3ee9ee94cd0\n",
      "  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18998 sha256=02cd5ff42a79ba0a0f61228103653542ef94b1180384c1b9007c49d5caa4cf82\n",
      "  Stored in directory: /root/.cache/pip/wheels/63/f9/dc/2dd16d3330e327236e4d407941975c42d5159d200cdb7922d8\n",
      "  Building wheel for cbor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cbor: filename=cbor-1.0.0-cp311-cp311-linux_x86_64.whl size=53974 sha256=05718a3d3383fd8d7abc2c7e3e5f9670c9da52f19d2bd0c1031646d9cd5768ee\n",
      "  Stored in directory: /root/.cache/pip/wheels/21/6b/45/0c34253b1af07d1d9dc524f6d44d74a6b191c43152e6aaf641\n",
      "Successfully built neural-cherche warc3-wet-clueweb09 cbor\n",
      "Installing collected packages: warc3-wet-clueweb09, warc3-wet, pytz, cbor, zlib-state, xxhash, unlzw3, tzdata, trec-car-tools, tqdm, threadpoolctl, scipy, safetensors, regex, pyarrow, propcache, multidict, lz4, lxml, joblib, ijson, hf-xet, frozenlist, dill, aiohappyeyeballs, yarl, scikit-learn, pandas, multiprocess, inscriptis, huggingface-hub, aiosignal, tokenizers, lenlp, ir_datasets, aiohttp, transformers, neural-cherche, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 cbor-1.0.0 datasets-4.1.1 dill-0.4.0 frozenlist-1.7.0 hf-xet-1.1.10 huggingface-hub-0.35.0 ijson-3.4.0 inscriptis-2.6.0 ir_datasets-0.5.11 joblib-1.5.2 lenlp-1.2.0 lxml-6.0.1 lz4-4.4.4 multidict-6.6.4 multiprocess-0.70.16 neural-cherche-1.4.3 pandas-2.3.2 propcache-0.3.2 pyarrow-21.0.0 pytz-2025.2 regex-2025.9.1 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.2 threadpoolctl-3.6.0 tokenizers-0.22.0 tqdm-4.67.1 transformers-4.56.1 trec-car-tools-2.6 tzdata-2025.2 unlzw3-0.2.3 warc3-wet-0.2.5 warc3-wet-clueweb09-0.2.5 xxhash-3.5.0 yarl-1.20.1 zlib-state-0.1.10\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets ir_datasets neural-cherche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee29ae2a-a8d6-4025-a147-fecb5fdfacd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'muvera-py'...\n",
      "remote: Enumerating objects: 10, done.\u001b[K\n",
      "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
      "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
      "remote: Total 10 (delta 0), reused 10 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (10/10), 1.91 MiB | 7.10 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sionic-ai/muvera-py.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a70ab56d-4b15-4adb-a631-5703f6262145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "MUVERA_PY_PATH = \"./muvera-py\"\n",
    "if not os.path.isdir(MUVERA_PY_PATH):\n",
    "    raise FileNotFoundError(f\"The directory '{MUVERA_PY_PATH}' was not found. Please clone the repository first.\")\n",
    "\n",
    "# Change current directory\n",
    "os.chdir(MUVERA_PY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd58d808-8ee3-4e50-b95c-0c9e4a403ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List\n",
    "from dataclasses import replace\n",
    "import json\n",
    "# Import third-party libraries\n",
    "from neural_cherche import models, rank\n",
    "import logging\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9ce8f13-5e8d-4109-bc5d-dbcf46a835a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def create_corpus():\n",
    "    with open('./corpus_v1.json', 'r') as file:\n",
    "      data = json.load(file)\n",
    "    # 2. Create the corpus dictionary\n",
    "    corpus = {}\n",
    "    for id,item in data.items():\n",
    "        corpus[id] = {\"title\": \"\", \"text\": item['text']}\n",
    "\n",
    "    # 3. Create the queries dictionary\n",
    "    queries = {}\n",
    "    # Assuming Queries.json contains a list of query strings or a dictionary\n",
    "    # Let's assume it's a list of dictionaries like [{'query': 'query text', 'query_id': '1'}, ...]\n",
    "    # If it's just a list of strings, we'll need to adjust how we generate query_ids\n",
    "    try:\n",
    "        with open('./Queries.json', 'r') as file:\n",
    "            rag_queries_data = json.load(file)\n",
    "        # Assuming rag_queries_data is a list of dictionaries, each with a 'query' key\n",
    "        for i, item in enumerate(rag_queries_data):\n",
    "             # Generate a simple query_id if not provided\n",
    "            query_id = item.get('query_id', f\"query_{i+1}\")\n",
    "            queries[query_id] = item['query']\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: /content/Queries.json not found. Queries dictionary will be empty.\")\n",
    "    except KeyError:\n",
    "        print(\"Error: 'query' key not found in one of the items in Queries.json.\")\n",
    "        # Handle the case where the JSON structure is unexpected\n",
    "        queries = {} # Reset queries to avoid further errors\n",
    "\n",
    "    return corpus,queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d2149-cbd7-4029-a435-aa08d960be3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at colbert-ir/colbertv2.0 and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColbertFdeRetriever initialized successfully.\n",
      "\n",
      "--- Starting Corpus Indexing ---\n",
      "Generating native multi-vector embeddings for 91411 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ColBERT documents embeddings: 100%|██████████| 2857/2857 [06:11<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-vector embedding generation took 371.79 seconds.\n",
      "Generating FDEs from ColBERT embeddings using the optimized BATCH function...\n",
      "FDE generation took 1048.20 seconds.\n",
      "--- Corpus Indexing Finished in 1419.99 seconds ---\n",
      "Final FDE Index Shape: (91411, 10240)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List\n",
    "from dataclasses import replace\n",
    "\n",
    "# Import third-party libraries\n",
    "from neural_cherche import models, rank\n",
    "\n",
    "\n",
    "# Import our custom modules\n",
    "from fde_generator import (\n",
    "    FixedDimensionalEncodingConfig,\n",
    "    generate_document_fde_batch,\n",
    "    generate_query_fde, # We need this for the search step\n",
    "    EncodingType,\n",
    ")\n",
    "import logging\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List\n",
    "from dataclasses import replace\n",
    "\n",
    "# Import third-party libraries\n",
    "from neural_cherche import models, rank\n",
    "from ir_datasets.util import Cache\n",
    "\n",
    "# Import our custom modules\n",
    "from fde_generator import (\n",
    "    FixedDimensionalEncodingConfig,\n",
    "    generate_document_fde_batch,\n",
    "    generate_query_fde, # We need this for the search step\n",
    "    EncodingType,\n",
    "    ProjectionType,\n",
    ")\n",
    "import logging\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "class ColbertFdeRetriever:\n",
    "    \"\"\"\n",
    "    Uses a real ColBERT model to generate embeddings, then FDE for search.\n",
    "    This version uses the correct `rank.ColBERT` interface.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"colbert-ir/colbertv2.0\"):\n",
    "        # --- Model Initialization ---\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model = models.ColBERT(model_name_or_path=model_name, device=device)\n",
    "\n",
    "        # We use the 'rank' API as shown in the original muvera-py repo\n",
    "        self.ranker = rank.ColBERT(key=\"id\", on=[\"title\", \"text\"], model=model)\n",
    "\n",
    "        # --- FDE Configuration ---\n",
    "        self.doc_config = FixedDimensionalEncodingConfig(\n",
    "                    dimension=128,\n",
    "                    num_repetitions=20,\n",
    "                    num_simhash_projections=5,\n",
    "                    projection_type=ProjectionType.AMS_SKETCH, # Enable the projection\n",
    "                    projection_dimension=16, # The new, smaller dimension\n",
    "                    seed=42,\n",
    "                    encoding_type=EncodingType.AVERAGE,\n",
    "                    fill_empty_partitions=True,\n",
    "                )\n",
    "\n",
    "        self.fde_index, self.doc_ids = None, []\n",
    "        self.doc_embeddings_list = None # Initialize the attribute here\n",
    "        print(\"ColbertFdeRetriever initialized successfully.\")\n",
    "\n",
    "    # In your main script (ColbertFdeRetriever class)\n",
    "\n",
    "    def index(self, corpus: dict):\n",
    "        \"\"\"\n",
    "        Generates FDEs for the corpus using the highly optimized batch function.\n",
    "        \"\"\"\n",
    "        print(\"\\n--- Starting Corpus Indexing ---\")\n",
    "        start_time = time.time()  \n",
    "\n",
    "        self.doc_ids = list(corpus.keys())\n",
    "        documents_for_ranker = [\n",
    "            {\"id\": doc_id, **corpus[doc_id]} for doc_id in self.doc_ids\n",
    "        ]\n",
    "\n",
    "        print(f\"Generating native multi-vector embeddings for {len(documents_for_ranker)} documents...\")\n",
    "        doc_embeddings_map = self.ranker.encode_documents(\n",
    "            documents=documents_for_ranker, batch_size=32\n",
    "        )\n",
    "        self.doc_embeddings_list = [\n",
    "            doc_embeddings_map[doc_id] for doc_id in self.doc_ids\n",
    "        ]\n",
    "\n",
    "        duration_embed = time.time() - start_time\n",
    "        print(f\"Multi-vector embedding generation took {duration_embed:.2f} seconds.\")\n",
    "\n",
    "        # --- REPLACEMENT LOGIC ---\n",
    "        print(\"Generating FDEs from ColBERT embeddings using the optimized BATCH function...\")\n",
    "        start_fde_time = time.time()\n",
    "\n",
    "        # A single, powerful function call replaces the entire multiprocessing block.\n",
    "        self.fde_index = generate_document_fde_batch(\n",
    "            self.doc_embeddings_list, self.doc_config\n",
    "        )\n",
    "\n",
    "        duration_fde = time.time() - start_fde_time\n",
    "        print(f\"FDE generation took {duration_fde:.2f} seconds.\")\n",
    "        print(f\"--- Corpus Indexing Finished in {time.time() - start_time:.2f} seconds ---\")\n",
    "        print(f\"Final FDE Index Shape: {self.fde_index.shape}\")\n",
    "\n",
    "\n",
    "    def search(self, query: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Encodes a query to an FDE and searches the document index.\n",
    "        \"\"\"\n",
    "        # Step 4c: Encode the query into a multi-vector embedding\n",
    "        query_embeddings_map = self.ranker.encode_queries(queries=[query])\n",
    "        query_embeddings = list(query_embeddings_map.values())[0]\n",
    "\n",
    "        # Step 4d: Convert the query embedding to an FDE\n",
    "        # We create a new config for the query. Crucially, fill_empty_partitions must be False.\n",
    "        query_config = replace(self.doc_config, fill_empty_partitions=False)\n",
    "        query_fde = generate_query_fde(query_embeddings, query_config)\n",
    "\n",
    "        # Step 4e: The Search!\n",
    "        # This is a single, lightning-fast matrix-vector multiplication.\n",
    "        # It computes the dot product of the query FDE against all document FDEs simultaneously.\n",
    "        scores = self.fde_index @ query_fde\n",
    "\n",
    "        # Return a dictionary of {doc_id: score} sorted from highest to lowest score.\n",
    "        return dict(\n",
    "            sorted(zip(self.doc_ids, scores), key=lambda item: item[1], reverse=True)\n",
    "        )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Step 1: Load Dataset ---\n",
    "    corpus,queries = create_corpus()\n",
    "\n",
    "    # --- Step 2 & 3: Initialize and Index ---\n",
    "    fde_retriever = ColbertFdeRetriever()\n",
    "    fde_retriever.index(corpus=corpus)\n",
    "\n",
    "    # # --- Step 4: Perform a Search ---\n",
    "    # print(\"\\n--- Performing Search ---\")\n",
    "\n",
    "    # # Let's take the first query from the dataset as an example\n",
    "    # example_query_id = list(queries.keys())[0]\n",
    "    # example_query_text = queries[example_query_id]\n",
    "\n",
    "    # print(f\"Searching for query ID '{example_query_id}': '{example_query_text}'\")\n",
    "\n",
    "    # search_start_time = time.time()\n",
    "    # results = fde_retriever.search(query=example_query_text)\n",
    "    # search_duration = time.time() - search_start_time\n",
    "\n",
    "    # print(f\"Search completed in {search_duration:.4f} seconds.\")\n",
    "\n",
    "    # # --- Display Top 5 Results ---\n",
    "    # print(\"\\nTop 5 results:\")\n",
    "    # for i, (doc_id, score) in enumerate(list(results.items())[:5]):\n",
    "    #     print(f\"{i+1}. Doc ID: {doc_id}, Score: {score:.4f}\")\n",
    "    #     # print(f\"   Title: {corpus[doc_id]['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b509ce85-0846-40ca-a573-fee316499d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Assuming `fde_retriever` has finished indexing...\n",
    "fde_index = fde_retriever.fde_index\n",
    "doc_ids = fde_retriever.doc_ids\n",
    "\n",
    "print(\"\\n--- Saving Index and Document IDs ---\")\n",
    "\n",
    "# 1. Save the FDE index to a binary .npy file\n",
    "np.save(\"fde_index.npy\", fde_index)\n",
    "print(f\"FDE index saved to fde_index.npy with shape {fde_index.shape}\")\n",
    "\n",
    "# 2. Save the document IDs to a JSON file\n",
    "with open(\"doc_ids.json\", \"w\") as f:\n",
    "    json.dump(doc_ids, f)\n",
    "print(\"Document IDs saved to doc_ids.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaa9735-bc04-4150-a003-c9407dfc9f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
